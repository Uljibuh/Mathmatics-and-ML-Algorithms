{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-22T11:55:53.918679Z","iopub.execute_input":"2022-02-22T11:55:53.919009Z","iopub.status.idle":"2022-02-22T11:55:53.925864Z","shell.execute_reply.started":"2022-02-22T11:55:53.918970Z","shell.execute_reply":"2022-02-22T11:55:53.924779Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**linear regression**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom numpy.random import randn\nimport random\nfrom IPython.core.display import display, Image\nfrom string import Template\nimport IPython.display\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2022-02-22T14:12:36.767373Z","iopub.execute_input":"2022-02-22T14:12:36.767642Z","iopub.status.idle":"2022-02-22T14:12:37.944267Z","shell.execute_reply.started":"2022-02-22T14:12:36.767612Z","shell.execute_reply":"2022-02-22T14:12:37.943498Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class LinearRegression:\n    \n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n        \n    \n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        \n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \n        # gradient descent\n        for _ in range(self.n_iters):\n            y_predicted = np.dot(X, self.weights) + self.bias\n            \n            # compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            \n            db = (1 / n_samples) * np.sum(y_predicted - y)\n            \n            # update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n            \n    def predict(self, X):\n        y_approximated = np.dot(X, self.weights) + self.bias\n        return y_approximated\n    \ndef mean_squared_error(y_true, y_pred):\n    return np.mean((y_true - y_pred)**2)\n\n\n\nX, y = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n# play with learning rate and n_iters\nregressor = LinearRegression(learning_rate=0.01, n_iters=1000) \nregressor.fit(X_train, y_train)\npredictions = regressor.predict(X_test)\n\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\n\ny_pred_line = regressor.predict(X)\n\ncmap = plt.get_cmap('viridis')\nfig = plt.figure(figsize=(8,6))\nm1 = plt.scatter(X_train, y_train, color=cmap(0.9), s=10)\nm2 = plt.scatter(X_test, y_test, color=cmap(0.5), s=10)\nplt.plot(X, y_pred_line, color='black', linewidth=2, label=\"prediction\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:18:58.977412Z","iopub.execute_input":"2022-02-22T13:18:58.977860Z","iopub.status.idle":"2022-02-22T13:18:59.298166Z","shell.execute_reply.started":"2022-02-22T13:18:58.977827Z","shell.execute_reply":"2022-02-22T13:18:59.297218Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Decision Tree**","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ndef entropy(y):\n  hist = np.bincount(y)\n  ps = hist/len(y)\n  return(-np.sum([p * np.log2(p) for p in ps if p>0]))\n\n\n\nclass Node:\n  def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n    self.feature = feature\n    self.threshold = threshold\n    self.left = left\n    self.right = right\n    self.value = value\n  \n  def is_leaf_node(self):\n    return(self.value is not None)\n\n\nclass DecisionTree:\n  def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n    self.min_samples_split = min_samples_split\n    self.max_depth = max_depth\n    self.n_feats = n_feats\n    self.root = None\n\n  def fit(self, X, y):\n    self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n    self.root = self._grow_tree(X, y)\n\n  def _grow_tree(self, X, y, depth=0):\n    n_samples, n_features = X.shape\n    n_labels = len(np.unique(y))\n\n    #stopping criteria\n    if(depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n      leaf_value = self._most_common_label(y)\n      return(Node(value=leaf_value))\n    \n    feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n\n    #greedy search\n    best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n\n    left_idxs, right_idxs = self._split(X[:,best_feat],best_thresh)\n\n    left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth+1)\n    right = self._grow_tree(X[right_idxs,:], y[right_idxs], depth+1)\n    return(Node(best_feat, best_thresh, left, right))\n  \n\n  def _best_criteria(self, X, y, feat_idxs):\n    best_gain = -1\n    split_idx, split_thresh = None, None\n    for feat_idx in feat_idxs:\n      X_column = X[:, feat_idx]\n      thresholds = np.unique(X_column)\n      for threshold in thresholds:\n        gain = self._information_gain(y, X_column, threshold)\n        if(gain>best_gain):\n          best_gain = gain\n          split_idx = feat_idx\n          split_thresh = threshold\n    return(split_idx, split_thresh)\n\n  def _information_gain(self, y, X_column, split_threh):\n    #parent entropy\n    parent_entropy = entropy(y)\n\n    #generate split\n    left_idxs, right_idxs = self._split(X_column, split_threh)\n    if(len(left_idxs == 0) or len(right_idxs)==0):\n      return 0\n\n    #weighted avg vhild entropy\n    n = len(y)\n    n_l, n_r = len(left_idxs), len(right_idxs)\n    e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n    child_entropy = (n_l/n)*e_l + (n_r/n)*e_r\n\n    #return ig\n    ig = parent_entropy - child_entropy\n\n    return ig\n  \n  def _split(self, X_column, split_threh):\n    left_idxs = np.argwhere(X_column <= split_threh).flatten()\n    right_idxs = np.argwhere(X_column > split_threh).flatten()\n    return(left_idxs, right_idxs)\n  \n  def predict(self, X):\n    #traverse tree\n    return(np.array([self._traverse_tree(x, self.root) for x in X]))\n\n  def _traverse_tree(self, x, node):\n    if(node.is_leaf_node()):\n      return(node.value)\n\n    if(x[node.feature] <= node.threshold):\n      return(self._traverse_tree(x, node.left))\n    return(self._traverse_tree(x, node.right))\n\n  def _most_common_label(self, y):\n    counter = Counter(y)\n    most_common = counter.most_common(1)[0][0]\n    return(most_common)\n\n#decision tree test\n\n\ndef accuracy(y_true, y_pred):\n  accuracy = np.sum(y_true == y_pred) / len(y_true)\n  return(accuracy)\n\n\n\ndata = datasets.load_breast_cancer()\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n\n\n\nclf = DecisionTree(max_depth=20)\nclf.fit(X_train, y_train)\n\n\n\ny_pred = clf.predict(X_test)\nacc = accuracy(y_test, y_pred)\n\nprint(\"Accuracy: \",acc)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:19:02.235327Z","iopub.execute_input":"2022-02-22T13:19:02.236210Z","iopub.status.idle":"2022-02-22T13:19:16.714100Z","shell.execute_reply.started":"2022-02-22T13:19:02.236159Z","shell.execute_reply":"2022-02-22T13:19:16.713117Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, y_pred)\nimport seaborn as sns\nsns.heatmap(cnf_matrix, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:19:36.171411Z","iopub.execute_input":"2022-02-22T13:19:36.171698Z","iopub.status.idle":"2022-02-22T13:19:36.568288Z","shell.execute_reply.started":"2022-02-22T13:19:36.171666Z","shell.execute_reply":"2022-02-22T13:19:36.567035Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Support-Vector Machine**","metadata":{}},{"cell_type":"code","source":"class SVM:\n    def __init__(self, learning_rate = 0.001, lambda_param=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n        \n        \n    def fit(self, X, y):\n        y_ = np.where(y<=0, -1,1)\n        n_samples, n_features = X.shape\n        \n        \n        self.w = np.zeros(n_features)\n        self.b = 0\n        \n        \n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >=1\n                \n                if condition:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n                else:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n                    self.b -= self.lr * y_[idx]\n       \n    \n    def predict(self, X):\n        linear_output = np.dot(X, self.w) - self.b\n        return(np.sign(linear_output))\n    \n    \n    \nX, y = datasets.make_blobs(n_samples=50, n_features=3, centers=3, cluster_std=1.05, random_state=40)\ny = np.where(y == 0, -1, 1)\n\nclf = SVM()\nclf.fit(X, y)\npredictions = clf.predict(X)\n\nprint(clf.w, clf.b)\n\n\n\ndef visualize_svm():\n     def get_hyperplane_value(x, w, b, offset):\n        return (-w[0] * x + b + offset) / w[1]\n    \n    \n\n     fig = plt.figure()\n     ax = fig.add_subplot(1,1,1)\n     plt.scatter(X[:,0], X[:,1], marker='o',c=y)\n\n     x0_1 = np.amin(X[:,0])\n     x0_2 = np.amax(X[:,0])\n\n     x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, 0)\n     x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, 0)\n\n     x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -1)\n     x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -1)\n\n     x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, 1)\n     x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, 1)\n\n     ax.plot([x0_1, x0_2],[x1_1, x1_2], 'y--')\n     ax.plot([x0_1, x0_2],[x1_1_m, x1_2_m], 'k')\n     ax.plot([x0_1, x0_2],[x1_1_p, x1_2_p], 'k')\n\n     x1_min = np.amin(X[:,1])\n     x1_max = np.amax(X[:,1])\n     ax.set_ylim([x1_min-3,x1_max+3])\n\n     plt.show()\n\nvisualize_svm() \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T14:12:43.875461Z","iopub.execute_input":"2022-02-22T14:12:43.875778Z","iopub.status.idle":"2022-02-22T14:12:44.475211Z","shell.execute_reply.started":"2022-02-22T14:12:43.875744Z","shell.execute_reply":"2022-02-22T14:12:44.474407Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(y, predictions)\nimport seaborn as sns\nsns.heatmap(cnf_matrix, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T14:13:44.296375Z","iopub.execute_input":"2022-02-22T14:13:44.296644Z","iopub.status.idle":"2022-02-22T14:13:44.645047Z","shell.execute_reply.started":"2022-02-22T14:13:44.296615Z","shell.execute_reply":"2022-02-22T14:13:44.644508Z"},"trusted":true},"execution_count":5,"outputs":[]}]}