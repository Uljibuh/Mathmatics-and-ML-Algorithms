{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-16T15:12:54.770370Z","iopub.execute_input":"2022-01-16T15:12:54.770869Z","iopub.status.idle":"2022-01-16T15:12:54.783719Z","shell.execute_reply.started":"2022-01-16T15:12:54.770824Z","shell.execute_reply":"2022-01-16T15:12:54.782712Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**content**\n\n* introduction\n\n* overview of the data set\n\n* logistic regression\n  * computation graph\n  * initializing parameters\n  * forward propagation\n    * sigmoid function\n    * loss function\n    * cost function\n  * optimazation algorithm with gradient descent\n    * backward propagation\n    * updating parameters\n  * logistic regression with sklearn\n\n\n* artificial neural network\n  * 2 layer neural network\n    * size of layers and initializing paramaeters weights and bias\n    * forward propagation\n    * loss function and cost function\n    * update parameters\n    * prediction with learnt parameters weights and bias\n    * create model\n  * l layer neural network\n    * implementing with keras library\n","metadata":{}},{"cell_type":"markdown","source":"We will use \"sign language digits data set\" for this tutorial.\n\nIn this data there are 2062 sign language digits images.\n\nAs you know digits are from 0 to 9. Therefore there are 10 unique sign.\n\nAt the beginning of tutorial we will use only sign 0 and 1 for simplicity.\n\nIn data, sign zero is between indexes 204 and 408. Number of zero sign is 205.\n\nAlso sign one is between indexes 822 and 1027. Number of one sign is 206. Therefore, we will use 205 samples from each classes(labels).\n\nNote: Actually 205 sample is very very very little for deep learning. But this is tutorial so it does not matter so much.\nLets prepare our X and Y arrays. X is image array (zero and one signs) and Y is label array (0 and 1).","metadata":{}},{"cell_type":"code","source":"# load data set\nx_l = np.load('/kaggle/input/sign-language-digits-dataset/X.npy')\nY_l = np.load('/kaggle/input/sign-language-digits-dataset/Y.npy')\n\nimg_size = 64\nplt.subplot(1, 2, 1)\nplt.imshow(x_l[260].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(x_l[900].reshape(img_size, img_size))\nplt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:16:23.361292Z","iopub.execute_input":"2022-01-16T15:16:23.361574Z","iopub.status.idle":"2022-01-16T15:16:23.579916Z","shell.execute_reply.started":"2022-01-16T15:16:23.361535Z","shell.execute_reply":"2022-01-16T15:16:23.578970Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# join a sequence of arrays along an row axis\nX = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0)\n# from 0 to 204 is zero sign and from 205 to 410 is one sign\nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\nprint(\" X shape: \" , X.shape)\nprint(\" Y shape: \" , Y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:22:54.456258Z","iopub.execute_input":"2022-01-16T15:22:54.456535Z","iopub.status.idle":"2022-01-16T15:22:54.467607Z","shell.execute_reply.started":"2022-01-16T15:22:54.456504Z","shell.execute_reply":"2022-01-16T15:22:54.466667Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"the shape of the x is (410, 64, 64)\n410 means that we have 412 images (zero and one signs)\n64 mean that our image size is 64x64 pixels\n\nthe shape of the Y is (410,1)\n410 means that we have 410 labels(0 and 1)\n\nlet split X and Y into train and test sets\npercentage of test size, test = 15% and tarin = 75%\n\nrandom_state = use same seed while randomizing, it always creates same test and tarin set if we call split function repeatedly.","metadata":{}},{"cell_type":"code","source":"# Then lets create x_train, y_train, x_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:40:54.175361Z","iopub.execute_input":"2022-01-16T15:40:54.175893Z","iopub.status.idle":"2022-01-16T15:40:54.185262Z","shell.execute_reply.started":"2022-01-16T15:40:54.175843Z","shell.execute_reply":"2022-01-16T15:40:54.184134Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"now we have 3D input array X, we need to make it flatten(2D) in order to use as input for iur first deep learning model\n\nour label array (Y) is a already flatten(2D) \n","metadata":{}},{"cell_type":"code","source":"X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:40:57.333447Z","iopub.execute_input":"2022-01-16T15:40:57.333765Z","iopub.status.idle":"2022-01-16T15:40:57.340381Z","shell.execute_reply.started":"2022-01-16T15:40:57.333731Z","shell.execute_reply":"2022-01-16T15:40:57.339711Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"we have 348 images and each image has 4096 pixels in image train array\n\nwe have 62 images and each image has 4096 pixel in images test arays\n\nnow we transpose\n\ntranspose() function changes the row elements into column elements and the column elements into row elements. The output of this function is a modified array of the original one.","metadata":{}},{"cell_type":"code","source":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\n\nprint(\"x train: \", x_train.shape)\nprint(\"x test: \", x_test.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"y test: \", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:53:24.491794Z","iopub.execute_input":"2022-01-16T15:53:24.492057Z","iopub.status.idle":"2022-01-16T15:53:24.500865Z","shell.execute_reply.started":"2022-01-16T15:53:24.492026Z","shell.execute_reply":"2022-01-16T15:53:24.499857Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"so far what we did\n\nchoose our label that were zero sign one\n\ncreate and flatten train and test sets\n\nthen tranpose the falttened arrays","metadata":{}},{"cell_type":"markdown","source":"**logistic regression**\n we use it for binary calssification\n \n**initializinf parameters**\n\nour imaghe ahs 4096 pixel in x_train\n\neach pixels have own weights\n\nthe firts steo is multiplying each pixels with their own weights\n\nweight are 0.01 and the weight array shape is (4096,1)\n\ninitial bias is 0\n\n","metadata":{}},{"cell_type":"code","source":"# initialize parameters\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1), 0.01)\n    b = 0.0\n    return w, b","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:31:14.997603Z","iopub.execute_input":"2022-01-16T16:31:14.998371Z","iopub.status.idle":"2022-01-16T16:31:15.003404Z","shell.execute_reply.started":"2022-01-16T16:31:14.998337Z","shell.execute_reply":"2022-01-16T16:31:15.002348Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"\nForward Propagation\n\nThe all steps from pixels to cost is called forward propagation\n\nz = (w.T)x + b => in this equation we know x that is pixel array, we know w (weights) and b (bias) so the rest is calculation. (T is transpose)\n\nThen we put z into sigmoid function that returns y_head(probability). When your mind is confused go and look at computation graph. Also equation of sigmoid function is in computation graph.\n\nThen we calculate loss(error) function.\n\nCost function is summation of all loss(error).\n\nLets start with z and the write sigmoid definition(method) that takes z as input parameter and returns y_head(probability)","metadata":{}},{"cell_type":"code","source":"# calculation of z\n# z =np.dot(w.T, x_train)+b\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:31:19.045964Z","iopub.execute_input":"2022-01-16T16:31:19.046698Z","iopub.status.idle":"2022-01-16T16:31:19.052136Z","shell.execute_reply.started":"2022-01-16T16:31:19.046634Z","shell.execute_reply":"2022-01-16T16:31:19.051403Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"y_head = sigmoid(0)\ny_head","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:06:09.694581Z","iopub.execute_input":"2022-01-16T16:06:09.695048Z","iopub.status.idle":"2022-01-16T16:06:09.703598Z","shell.execute_reply.started":"2022-01-16T16:06:09.694997Z","shell.execute_reply":"2022-01-16T16:06:09.702436Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# forward propagation steps\n# find z = w.T*x+b\n# y_head = sigmoid(z)\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\n\ndef forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1] # x_train.shape is for scaling\n    return cost","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:31:21.425996Z","iopub.execute_input":"2022-01-16T16:31:21.426275Z","iopub.status.idle":"2022-01-16T16:31:21.432306Z","shell.execute_reply.started":"2022-01-16T16:31:21.426244Z","shell.execute_reply":"2022-01-16T16:31:21.431341Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"Optimization Algorithm with Gradient Descent\n\nnow we know what is our cost that is error.\n\nTherefore, we need to decrease cost because as we know if cost is high it means that we make wrong prediction.\n\nLets think first step, every thing starts with initializing weights and bias. Therefore cost is dependent with them.\n\nIn order to decrease cost, we need to update weights and bias.\n\nIn other words, our model needs to learn the parameters weights and bias that minimize cost function. This technique is called gradient descent.","metadata":{}},{"cell_type":"code","source":"# in backward propagation we use y_head thta found in forward propagation\n# lets combine forward and backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:31:24.904039Z","iopub.execute_input":"2022-01-16T16:31:24.904560Z","iopub.status.idle":"2022-01-16T16:31:24.913243Z","shell.execute_reply.started":"2022-01-16T16:31:24.904510Z","shell.execute_reply":"2022-01-16T16:31:24.912422Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Initializing parameters (implemented)\n\nFinding cost with forward propagation and cost function (implemented)\n\nUpdating(learning) parameters (weight and bias). Now lets implement it.","metadata":{}},{"cell_type":"code","source":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:31:29.004252Z","iopub.execute_input":"2022-01-16T16:31:29.005080Z","iopub.status.idle":"2022-01-16T16:31:29.014860Z","shell.execute_reply.started":"2022-01-16T16:31:29.005029Z","shell.execute_reply":"2022-01-16T16:31:29.013712Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":" # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:31:32.125693Z","iopub.execute_input":"2022-01-16T16:31:32.126001Z","iopub.status.idle":"2022-01-16T16:31:32.132486Z","shell.execute_reply.started":"2022-01-16T16:31:32.125969Z","shell.execute_reply":"2022-01-16T16:31:32.131629Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# put them all together\n\n     \ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:31:36.454128Z","iopub.execute_input":"2022-01-16T16:31:36.454637Z","iopub.status.idle":"2022-01-16T16:31:37.312814Z","shell.execute_reply.started":"2022-01-16T16:31:36.454576Z","shell.execute_reply":"2022-01-16T16:31:37.311716Z"},"trusted":true},"execution_count":47,"outputs":[]}]}